{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b870382e",
   "metadata": {},
   "source": [
    "### Do the whisper transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0778bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os\n",
    "from g2p_en import G2p\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List, Tuple\n",
    "import jiwer\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "# I don't think you need this but if you have issues uncomment it\n",
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "999d26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your audios\n",
    "PARTICIPANT_AUDIOS = \"/home/enchanted/Matcha-TTS/tts-outputs/\"\n",
    "# path to a csv that has the file name and transcript\n",
    "PARTICIPANT_TRANSCRIPTIONS = \"/home/paige/Documents/BB_pepper-experiment/analysis/test.csv\"\n",
    "model = whisper.load_model(\"medium.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa82156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_transcriptions = {}\n",
    "\n",
    "# Get a disctionary with : {filename: transcription}\n",
    "for filename in os.listdir(PARTICIPANT_AUDIOS):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        result = model.transcribe(f\"{PARTICIPANT_AUDIOS}/{filename}\")\n",
    "        asr_transcriptions[filename] = result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53c345e",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3e08ad",
   "metadata": {},
   "source": [
    "### Normalize the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1b30fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_g2p = G2p()\n",
    "\n",
    "\n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of words\n",
    "    Ignores punctuation and numbers\n",
    "    \"\"\"\n",
    "    # Lowercase, keep only words and apostrophes/hyphens inside words\n",
    "    return re.findall(r\"[a-zA-Z]+(?:['-][a-zA-Z]+)?\", text.lower())\n",
    "\n",
    "@lru_cache(maxsize=100_000)\n",
    "def word_to_phone_token(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Return a robust phonetic code for a single word.\n",
    "    Priority: g2p_en (ARPAbet phones)\n",
    "    The output is a compact string that's stable for equality checks.\n",
    "    \"\"\"\n",
    "    # ARPAbet phones from g2p_en; strip stress digits (AH0->AH) so homophones match\n",
    "    phones = [p for p in _g2p(word) if re.match(r\"^[A-Z]+[0-9]?$\", p)]\n",
    "    if phones:\n",
    "        phones = [re.sub(r\"\\d\", \"\", p) for p in phones]\n",
    "        # join phones with '-' so the whole word is ONE token for jiwer\n",
    "        return \"PH:\" + \"-\".join(phones)\n",
    "    # If g2p can't produce phones, mark as literal word so it won't silently match\n",
    "    return \"W:\" + word\n",
    "\n",
    "\n",
    "def phonetic_transform(s: str) -> str:\n",
    "    return \" \".join(word_to_phone_token(w) for w in tokenize_text(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79135c1",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39159198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_errors(\n",
    "    data: Dict[str, List[Tuple[str, str]]]\n",
    ") -> List[Dict[str, object]]:\n",
    "    rows = []\n",
    "    for fname, pairs in data.items():\n",
    "        for i, (ref, hyp) in enumerate(pairs, 1):\n",
    "            ref_phon = phonetic_transform(ref)\n",
    "            hyp_phon = phonetic_transform(hyp)\n",
    "\n",
    "            # Compute WER on the transformed strings (no extra kwargs)\n",
    "            er = jiwer.wer(ref_phon, hyp_phon)\n",
    "\n",
    "            if er > 0.0:\n",
    "                rows.append({\n",
    "                    \"file\": fname,\n",
    "                    \"original\": ref,\n",
    "                    \"transcription\": hyp,\n",
    "                    \"error_rate\": round(er, 3),\n",
    "                })\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a373e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file': 'test-ja-000.wav', 'original': 'yes I see you', 'transcription': ' duh', 'error_rate': 1.0}, {'file': 'test-def-000.wav', 'original': 'Hello! How is going? Your part should be done in a couple of days!', 'transcription': \" Hello, how are you? Listen to me if I'm not too old for you.\", 'error_rate': 0.857}]\n"
     ]
    }
   ],
   "source": [
    "# align the original text and transcriptions on file name\n",
    "transcription_pairs = {}\n",
    "\n",
    "origin_transcriptions = pd.read_csv(PARTICIPANT_TRANSCRIPTIONS)\n",
    "\n",
    "# Build the dataset structure\n",
    "transcription_pairs = defaultdict(list)\n",
    "\n",
    "for _, row in origin_transcriptions.iterrows():\n",
    "    filename = row['filename']\n",
    "    origin_transcript = row['transcription']\n",
    "    asr_transcript = asr_transcriptions.get(filename, \"\")\n",
    "    transcription_pairs[filename].append((origin_transcript, asr_transcript))\n",
    "\n",
    "errors = find_errors(transcription_pairs)\n",
    "# can export to csv if you want\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc5eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "origin_transcriptions = pd.read_csv(PARTICIPANT_TRANSCRIPTIONS)\n",
    " \n",
    "origin_transcriptions[\"wer\"] = 0.0\n",
    "origin_transcriptions[\"match\"] = \"\"\n",
    " \n",
    "for idx, row in origin_transcriptions.iterrows():\n",
    "    ref = row[\"ref\"]\n",
    "    hyp = row[\"asr\"]\n",
    " \n",
    "    origin_transcriptions.at[idx, \"wer\"] = find_errors(ref, hyp)\n",
    "    origin_transcriptions[\"match\"] = origin_transcriptions[\"wer\"].apply(\n",
    "        lambda x: \"yes\" if x > 0.0 else \"no\"\n",
    "    )\n",
    " \n",
    "origin_transcriptions.to_csv(\"aligned_transcriptions_with_match.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breakingb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
